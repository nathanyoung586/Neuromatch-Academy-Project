{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIvhc_R1D6k1"
   },
   "source": [
    "# Human Connectome Project (HCP) Dataset loader. Saves out individual subject CSV files.\n",
    "\n",
    "The HCP dataset comprises resting-state and task-based fMRI from a large sample of human subjects. The NMA-curated dataset includes time series data that has been preprocessed and spatially-downsampled by aggregating within 360 regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXIw61Dk-M5E"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "4eJOYQqgSMKV"
   },
   "outputs": [],
   "source": [
    "#@title Figure settings\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSdEhS5jKzkb"
   },
   "source": [
    "# Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oL4crLoCzkzk"
   },
   "outputs": [],
   "source": [
    "# The download cells will store the data in nested directories starting here:\n",
    "HCP_DIR = \"./hcp\"\n",
    "if not os.path.isdir(HCP_DIR):\n",
    "  os.mkdir(HCP_DIR)\n",
    "\n",
    "# The data shared for NMA projects is a subset of the full HCP dataset\n",
    "N_SUBJECTS = 339\n",
    "\n",
    "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
    "N_PARCELS = 360\n",
    "\n",
    "# The acquisition parameters for all tasks were identical\n",
    "TR = 0.72  # Time resolution, in sec\n",
    "\n",
    "# The parcels are matched across hemispheres with the same order\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "\n",
    "# Each experiment was repeated multiple times in each subject\n",
    "N_RUNS_REST = 4\n",
    "N_RUNS_TASK = 2\n",
    "\n",
    "# Time series data are organized by experiment, with each experiment\n",
    "# having an LR and RL (phase-encode direction) acquistion\n",
    "BOLD_NAMES = [\n",
    "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
    "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
    "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
    "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
    "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
    "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
    "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
    "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
    "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
    "]\n",
    "\n",
    "# You may want to limit the subjects used during code development.\n",
    "# This will use all subjects:\n",
    "subjects = range(N_SUBJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErP_ocaxK9FU"
   },
   "source": [
    "# Downloading data\n",
    "\n",
    "The rest and task data are shared in different files, but they will unpack into the same directory structure.\n",
    "\n",
    "Each file is fairly large and will take some time to download. If you are focusing only on rest or task analyses, you may not want to download only that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Lh1SEyZ_Kdh"
   },
   "outputs": [],
   "source": [
    "fname = \"hcp_rest.tgz\"\n",
    "import wget\n",
    "if not os.path.exists(fname):\n",
    "  !wget -qO $fname https://osf.io/bqp7m/download/\n",
    "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ubuyGN7siwI"
   },
   "outputs": [],
   "source": [
    "fname = \"hcp_task.tgz\"\n",
    "if not os.path.exists(fname):\n",
    "  !wget -qO $fname https://osf.io/s4h8j/download/\n",
    "  !tar -xzf $fname -C $HCP_DIR --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaF8v-UBTcxq"
   },
   "source": [
    "## Loading region information\n",
    "\n",
    "Downloading either dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
    "\n",
    "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
    "\n",
    "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28q_GDlXOyMi"
   },
   "outputs": [],
   "source": [
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    myelin=regions[2].astype(np.float),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NYDxvWrbIxxk"
   },
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDFPnQ07MmEd"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnq7H5h_IxLi"
   },
   "outputs": [],
   "source": [
    "def get_image_ids(name):\n",
    "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
    "\n",
    "    Args:\n",
    "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    Returns:\n",
    "      run_ids (list of int) : Numeric ID for experiment image files\n",
    "\n",
    "  \"\"\"\n",
    "  run_ids = [\n",
    "    i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
    "  ]\n",
    "  if not run_ids:\n",
    "    raise ValueError(f\"Found no data for '{name}''\")\n",
    "  return run_ids\n",
    "\n",
    "def load_timeseries(subject, name, runs=None, concat=True, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
    "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
    "      or None to load all runs.\n",
    "    concat (bool) : If True, concatenate multiple runs in time\n",
    "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  # Get the list relative 0-based index of runs to use\n",
    "  if runs is None:\n",
    "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
    "  elif isinstance(runs, int):\n",
    "    runs = [runs]\n",
    "\n",
    "  # Get the first (1-based) run id for this experiment \n",
    "  offset = get_image_ids(name)[0]\n",
    "\n",
    "  # Load each run's data\n",
    "  bold_data = [\n",
    "      load_single_timeseries(subject, offset + run, remove_mean) for run in runs\n",
    "  ]\n",
    "\n",
    "  # Optionally concatenate in time\n",
    "  if concat:\n",
    "    bold_data = np.concatenate(bold_data, axis=-1)\n",
    "\n",
    "  return bold_data\n",
    "\n",
    "\n",
    "def load_single_timeseries(subject, bold_run, remove_mean=True):\n",
    "  \"\"\"Load timeseries data for a single subject and single run.\n",
    "  \n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    bold_run (int): 1-based run index, across all tasks\n",
    "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "\n",
    "  Returns\n",
    "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "\n",
    "  \"\"\"\n",
    "  bold_path = f\"{HCP_DIR}/subjects/{subject}/timeseries\"\n",
    "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
    "  ts = np.load(f\"{bold_path}/{bold_file}\")\n",
    "  if remove_mean:\n",
    "    ts -= ts.mean(axis=1, keepdims=True)\n",
    "  return ts\n",
    "\n",
    "def load_evs(subject, name, condition):\n",
    "  \"\"\"Load EV (explanatory variable) data for one task condition.\n",
    "\n",
    "  Args:\n",
    "    subject (int): 0-based subject ID to load\n",
    "    name (str) : Name of task\n",
    "    condition (str) : Name of condition\n",
    "\n",
    "  Returns\n",
    "    evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
    "      of the condition for each run.\n",
    "\n",
    "  \"\"\"\n",
    "  evs = []\n",
    "  #Check if file is empty\n",
    "\n",
    "        \n",
    "  for id in get_image_ids(name):\n",
    "    task_key = BOLD_NAMES[id - 1]\n",
    "    ev_file = f\"{HCP_DIR}/subjects/{subject}/EVs/{task_key}/{condition}.txt\"\n",
    "    if os.stat(ev_file).st_size != 0:\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "        evs.append(ev)\n",
    "    else:\n",
    "        evs.append(dict(zip([\"onset\", \"duration\", \"amplitude\"], np.array([[],[],[]]))))\n",
    "  return evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQzCA99sMryW"
   },
   "source": [
    "## Task-based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgnEuN0gMqxP"
   },
   "outputs": [],
   "source": [
    "def condition_frames(run_evs, skip=0):\n",
    "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
    "\n",
    "  Args:\n",
    "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
    "\n",
    "  \"\"\"\n",
    "  frames_list = []\n",
    "\n",
    "  for ev in run_evs:\n",
    "    \n",
    "    if np.shape(ev['onset'])[0]!=0:\n",
    "\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
    "\n",
    "        frames_list.append(np.concatenate(frames))\n",
    "    \n",
    "    else:\n",
    "        frames_list.append([])\n",
    "\n",
    "\n",
    "  return frames_list\n",
    "\n",
    "\n",
    "def selective_average(timeseries_data, ev, skip=0):\n",
    "  \"\"\"Take the temporal mean across frames for a given condition.\n",
    "\n",
    "  Args:\n",
    "    timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
    "    ev (dict or list of dicts): Condition timing information\n",
    "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
    "      for hemodynamic lag\n",
    "\n",
    "  Returns:\n",
    "    avg_data (1D array): Data averagted across selected image frames based\n",
    "    on condition timing\n",
    "\n",
    "  \"\"\"\n",
    "  # Ensure that we have lists of the same length\n",
    "  if not isinstance(timeseries_data, list):\n",
    "    timeseries_data = [timeseries_data]\n",
    "  if not isinstance(ev, list):\n",
    "    ev = [ev]\n",
    "  if len(timeseries_data) != len(ev):\n",
    "    raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
    "\n",
    "  # Identify the indices of relevant frames\n",
    "  frames = condition_frames(ev)\n",
    "\n",
    "  # Select the frames from each image\n",
    "  selected_data = []\n",
    "  for run_data, run_frames in zip(timeseries_data, frames):\n",
    "    selected_data.append(run_data[:, run_frames])\n",
    "\n",
    "  # Take the average in each parcel\n",
    "  avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
    "\n",
    "  return avg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create all-task CSVs for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-058325ac9cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata_stacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdf_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_stacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpd_subj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_subj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mpd_subj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%i_all_task.csv'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msubj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7079\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7080\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7081\u001b[0;31m         return concat(\n\u001b[0m\u001b[1;32m   7082\u001b[0m             \u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7083\u001b[0m             \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             new_data = concatenate_block_managers(\n\u001b[0m\u001b[1;32m    497\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             b = make_block(\n\u001b[0;32m-> 2022\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     to_concat = [\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     to_concat = [\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_categorical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;31m# External code requested filling/upcasting, bool values must\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# be upcasted to object to avoid being upcasted to numeric.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mis_bool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2606\u001b[0m         \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m         \"\"\"\n\u001b[0;32m-> 2608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m     def convert(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task_names = ['rest', 'motor', 'wm', 'emotion', 'gambling', 'language', 'relational', 'social']\n",
    "task_nos = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "columns = np.hstack((np.array(['subject', 'task_name', 'task_num']),region_info['name']))\n",
    "pd_subj = pd.DataFrame(columns=columns)\n",
    "\n",
    "for subj in subjects:\n",
    "    pd_subj = pd.DataFrame(columns=columns)\n",
    "    for i_t, t in enumerate(task_names):\n",
    "        # homework: figure out how to append trial number too\n",
    "        # use t for task name, i_t to get integer for task number\n",
    "        data = load_timeseries(subj, t, concat=True).T\n",
    "        n_trials = data.shape[0]\n",
    "        info = np.tile(np.array([subj, t, i_t]), n_trials).reshape(n_trials, -1)\n",
    "        data_stacked = np.hstack((info, data))\n",
    "        df_task = pd.DataFrame(data_stacked, columns = columns)\n",
    "        pd_subj = pd_subj.append(df_task, ignore_index=True)\n",
    "    pd_subj.to_csv('%i_all_task.csv'%subj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make subject dataframes and CSVs for boredom analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_names = ['rest', 'wm']\n",
    "task_nos = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "columns = np.hstack((np.array(['subject', 'task_name', 'task_num', 'run', 'condition', 'frame_num','trial_num', 'correct_or_not', 'classifier_correct', 'classifer_prediction', 'predict_prob_WM', 'predict_prob_rest']), region_info['name']))\n",
    "pd_subj = pd.DataFrame(columns=columns)\n",
    "\n",
    "for subj in subjects:\n",
    "    pd_subj = pd.DataFrame(columns=columns)\n",
    "    for i_t, t in enumerate(task_names):\n",
    "        if t == 'wm':\n",
    "            for run in range(2):\n",
    "                data = load_timeseries(subj, t, runs=run, concat=True).T\n",
    "                n_frames = data.shape[0]\n",
    "                info = np.tile(np.array([subj, t, i_t, run+1, 0, 0, 0, 0, 0, 0, 0, 0 ]), n_frames).reshape(n_frames, -1)\n",
    "                data_stacked = np.hstack((info, data))\n",
    "                df_task = pd.DataFrame(data_stacked, columns = columns)\n",
    "                df_task['frame_num'] = range(len(df_task))\n",
    "                pd_subj = pd_subj.append(df_task, ignore_index=True)\n",
    "#                 pd_subj.loc[(pd_subj['run']==run+1)&(pd_subj['task_name']=='wm'), 'frame_num'] = range(len(pd_subj.loc[(pd_subj['run']==run+1)&(pd_subj['task_name']=='wm')]))\n",
    "        elif t =='rest':\n",
    "            data = load_timeseries(subj, t, concat=True).T\n",
    "            n_frames = data.shape[0]\n",
    "            info = np.tile(np.array([subj, t, i_t, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]), n_frames).reshape(n_frames, -1)\n",
    "            data_stacked = np.hstack((info, data))\n",
    "            df_task = pd.DataFrame(data_stacked, columns = columns)\n",
    "            pd_subj = pd_subj.append(df_task, ignore_index=True)\n",
    "            pd_subj.loc[pd_subj['task_name']=='rest', 'frame_num'] = range(len(pd_subj.loc[pd_subj['task_name']=='rest']))\n",
    "        \n",
    "        if t == 'wm':\n",
    "            bk_0_cor = load_evs(subj,'wm','0bk_cor')\n",
    "            bk_0_cor_frame_indices_run1 = condition_frames(bk_0_cor)[0]\n",
    "            bk_0_cor_frame_indices_run2 = condition_frames(bk_0_cor)[1]\n",
    "\n",
    "            bk_0_err = load_evs(subj,'wm','0bk_err')\n",
    "            bk_0_err_frame_indices_run1 = condition_frames(bk_0_err)[0]\n",
    "            bk_0_err_frame_indices_run2 = condition_frames(bk_0_err)[1]\n",
    "\n",
    "            bk_0_nlr = load_evs(subj,'wm','0bk_nlr')\n",
    "            bk_0_nlr_frame_indices_run1 = condition_frames(bk_0_nlr)[0]\n",
    "            bk_0_nlr_frame_indices_run2 = condition_frames(bk_0_nlr)[1]\n",
    "\n",
    "            bk_2_cor = load_evs(subj,'wm','2bk_cor')\n",
    "            bk_2_cor_frame_indices_run1 = condition_frames(bk_2_cor)[0]  \n",
    "            bk_2_cor_frame_indices_run2 = condition_frames(bk_2_cor)[1]         \n",
    "\n",
    "            bk_2_err = load_evs(subj,'wm','2bk_err')\n",
    "            bk_2_err_frame_indices_run1 = condition_frames(bk_2_err)[0]    \n",
    "            bk_2_err_frame_indices_run2 = condition_frames(bk_2_err)[1]         \n",
    "\n",
    "            bk_2_nlr = load_evs(subj,'wm','2bk_nlr')\n",
    "            bk_2_nlr_frame_indices_run1 = condition_frames(bk_2_nlr)[0]\n",
    "            bk_2_nlr_frame_indices_run2 = condition_frames(bk_2_nlr)[1]\n",
    "\n",
    "    \n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_cor_frame_indices_run1), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_cor_frame_indices_run1), 'correct_or_not'] = 'cor'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_err_frame_indices_run1), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_err_frame_indices_run1) & (pd_subj.correct_or_not != 'cor'), 'correct_or_not'] = 'err'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_nlr_frame_indices_run1), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_0_nlr_frame_indices_run1) & (pd_subj.correct_or_not != 'cor') & (pd_subj.correct_or_not != 'nlr'), 'correct_or_not'] = 'nlr'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_cor_frame_indices_run2), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_cor_frame_indices_run2), 'correct_or_not'] = 'cor'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_err_frame_indices_run2), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_err_frame_indices_run2) & (pd_subj.correct_or_not != 'cor'), 'correct_or_not'] = 'err'\n",
    "    \n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_nlr_frame_indices_run2), 'condition'] = '0_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_0_nlr_frame_indices_run2) & (pd_subj.correct_or_not != 'cor') & (pd_subj.correct_or_not != 'nlr'), 'correct_or_not'] = 'nlr'\n",
    "\n",
    "    \n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_cor_frame_indices_run1), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_cor_frame_indices_run1), 'correct_or_not'] = 'cor'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_err_frame_indices_run1), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_err_frame_indices_run1) & (pd_subj.correct_or_not != 'cor'), 'correct_or_not'] = 'err'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_nlr_frame_indices_run1), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='1') & pd_subj.frame_num.isin(bk_2_nlr_frame_indices_run1) & (pd_subj.correct_or_not != 'cor') & (pd_subj.correct_or_not != 'nlr'), 'correct_or_not'] = 'nlr'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_cor_frame_indices_run2), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_cor_frame_indices_run2), 'correct_or_not'] = 'cor'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_err_frame_indices_run2), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_err_frame_indices_run2) & (pd_subj.correct_or_not != 'cor'), 'correct_or_not'] = 'err'\n",
    "\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_nlr_frame_indices_run2), 'condition'] = '2_bk'\n",
    "    pd_subj.loc[(pd_subj.run=='2') & pd_subj.frame_num.isin(bk_2_nlr_frame_indices_run2) & (pd_subj.correct_or_not != 'cor') & (pd_subj.correct_or_not != 'nlr'), 'correct_or_not'] = 'nlr'\n",
    "    \n",
    "    sorted_onsets = [[],[]]\n",
    "    sorted_durations = [[],[]]\n",
    "    \n",
    "    sorted_onsets[0] = np.sort(np.hstack((bk_0_cor[0]['onset'], bk_0_err[0]['onset'], bk_0_nlr[0]['onset'], bk_2_cor[0]['onset'], bk_2_err[0]['onset'], bk_2_nlr[0]['onset'])))\n",
    "    sorted_durations[0] = np.sort(np.hstack((bk_0_cor[0]['duration'], bk_0_err[0]['duration'], bk_0_nlr[0]['duration'], bk_2_cor[0]['duration'], bk_2_err[0]['duration'], bk_2_nlr[0]['duration'])))\n",
    "\n",
    "    sorted_onsets[1] = np.sort(np.hstack((bk_0_cor[1]['onset'], bk_0_err[1]['onset'], bk_0_nlr[1]['onset'], bk_2_cor[1]['onset'], bk_2_err[1]['onset'], bk_2_nlr[1]['onset'])))\n",
    "    sorted_durations[1] = np.sort(np.hstack((bk_0_cor[1]['duration'], bk_0_err[1]['duration'], bk_0_nlr[1]['duration'], bk_2_cor[1]['duration'], bk_2_err[1]['duration'], bk_2_nlr[1]['duration'])))\n",
    "    \n",
    "    for run in range(2):\n",
    "        \n",
    "        frames_list = []\n",
    "\n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(sorted_onsets[run] / TR).astype(int)\n",
    "\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(sorted_durations[run] / TR).astype(int)\n",
    "\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "\n",
    "        frames_list.append(np.concatenate(frames))\n",
    "\n",
    "        for trial in range(len(sorted_onsets[run])):\n",
    "            \n",
    "            pd_subj.loc[(pd_subj.run==str(run+1))&(pd_subj.frame_num.isin(frames_list[0][trial*4:(trial+1)*4])), 'trial_num'] = trial+1\n",
    "\n",
    "        \n",
    "    pd_subj.to_csv('%i_boredom_df.csv'%subj)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZDFPnQ07MmEd",
    "qQzCA99sMryW"
   ],
   "name": "Copy of load_hcp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
